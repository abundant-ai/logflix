# Terminal-Bench S3 Data Structure

## Overview
Terminal-bench run data is stored in S3 with a hierarchical structure designed for easy organization and retrieval of benchmark results.

## S3 Path Structure
```
<bucket-name>/tb-2.0-audit/<date>/<task-id>/<model-name>/
```

### Path Components
- **bucket-name**: S3 bucket (e.g., `t-bench-mam`)
- **date**: Upload date in YYYY-MM-DD format (today's date when uploaded)
- **task-id**: Unique identifier for the benchmark task (e.g., `hello-world`, `oom`, `slurm-simple-node-monitoring`)
- **model-name**: AI model identifier with slashes replaced by underscores (e.g., `together_ai_Qwen_Qwen2.5-7B-Instruct-Turbo`)

## Files in Each Task Directory

### Core Files
1. **`agent.cast`** - Asciinema recording of the agent's terminal session
   - JSON format with header + timestamped terminal events
   - Header: `{"version": 2, "width": 160, "height": 40, "timestamp": 1752260565, "env": {...}}`
   - Event types:
     - `[timestamp, "i", "terminal_input"]` - User/agent input (keystrokes)
     - `[timestamp, "o", "terminal_output"]` - Terminal output (command results)
     - `[timestamp, "m", "escaped_json"]` - Agent metadata/thinking (JSON string)
   - The "m" events contain escaped JSON with agent reasoning:
     - `state_analysis`: Agent's understanding of current state
     - `explanation`: Agent's plan for next actions
     - `commands`: Planned commands with timeouts
     - `is_task_complete`: Whether agent thinks task is done
   - Contains complete terminal interaction, output, timing, AND agent thought process
   - Can be played back with asciinema player or parsed for analysis

2. **`task.yaml`** - Task configuration and metadata
   - YAML format containing task instructions, difficulty, tags
   - Example structure:
     ```yaml
     instruction: |
       Task description
     author_name: Name
     author_email: email@example.com
     difficulty: easy|medium|hard
     category: category-name
     tags: [tag1, tag2]
     parser_name: pytest
     max_agent_timeout_sec: 360.0
     ```

3. **`results.json`** - Run results and metadata
   - JSON format with run statistics, timing, success/failure
   - Contains run_id, model_name, accuracy, start/end times
   - Example fields: `accuracy`, `run_id`, `model_name`, `start_time`, `end_time`

### Analysis Files
4. **`task.check`** - Quality assessment output
   - Text output from `tb tasks check <task-id>`
   - LLM-generated analysis of task quality and completeness
   - May contain suggestions for task improvements

5. **`task.debug`** - Debug analysis output
   - Text output from `tb tasks debug <task-id> --run-id <run-id>`
   - Analysis of task failures and instruction sufficiency
   - Helps identify why agents might struggle with the task


## Data Relationships
- Each **run** can contain multiple **tasks**
- Each **task** generates its own set of files under its specific S3 prefix
- The **date** represents when the data was uploaded to S3, not when the original run occurred
- **model-name** is sanitized (slashes → underscores) for filesystem compatibility

## Example S3 Structure
```
t-bench-mam/
└── tb-2.0-audit/
    └── 2025-08-31/
        ├── hello-world/
        │   └── Oracle/
        │       ├── agent.cast
        │       ├── task.yaml
        │       ├── results.json
        │       ├── task.check
        │       └── task.debug
        └── slurm-simple-node-monitoring/
            └── together_ai_Qwen_Qwen2.5-7B-Instruct-Turbo/
                ├── agent.cast
                ├── task.yaml
                ├── results.json
                ├── task.check
                └── task.debug
```

## Usage for Viewer Development
- Use the S3 path structure to build navigation hierarchies (date → task → model)
- `agent.cast` files can be embedded using asciinema-player for terminal playback
- `results.json` provides metadata for filtering and sorting
- `task.yaml` gives context about task difficulty and requirements
- `task.check` and `task.debug` provide analysis insights for debugging views